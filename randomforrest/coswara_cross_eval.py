# -*- coding: utf-8 -*-
"""coswara_cross_eval

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10kmPrgy84LcOkjsz5TrLYfyk8ZZqs1uN
"""

import os 
import time
import glob
import pickle
import librosa
import zipfile
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import auc
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
sns.set() # Use seaborn's default style to make attractive graphs
sns.set_style("white")
sns.set_style("ticks")

# download all train-val datasets
print('Downloading zip ... started')
!wget https://osf.io/yxwjs/download
print('Downloading zip ... complete')

print('Unzipping ... started')
file_name = '/content/download'
with zipfile.ZipFile(file_name) as file:
    file.extractall()
print('Unzipping ... complete')

# Download cambridge test set (seprate from above)
print('Downloading zip ... started')
!wget https://osf.io/k8t23/download
print('Downloading zip ... complete')

print('Unzipping ... started')
file_name = '/content/download.1'
with zipfile.ZipFile(file_name) as file:
    file.extractall()
print('Unzipping ... complete')

# function defs
def make_train_val_split(dataset_type, file_list):
  train_data = []
  val_data = []

  train_labels = []
  val_labels = []  

  for key in ['_pos', '_neg']:
    temp = file_list[dataset_type+key]
    indx = np.arange(0,len(temp),1)
    np.random.shuffle(indx)
    for i in range(len(indx)):
      if i < int(0.7*len(indx)):
        file_data = open(temp[indx[i]],'rb')
        data = pickle.load(file_data)
        train_data.append(data.values)
        if key == '_pos':
          train_labels.append(1)
        else:
          train_labels.append(0)
      else:
        file_data = open(temp[indx[i]],'rb')
        data = pickle.load(file_data)
        val_data.append(data.values)        
        if key == '_pos':
          val_labels.append(1)
        else:
          val_labels.append(0)        
  return np.stack(train_data), np.stack(val_data), np.stack(train_labels), np.stack(val_labels)


def make_test(file_list):

  test_data = []
  for i in range(len(file_list)):
    file_data = open(file_list[i],'rb')
    data = pickle.load(file_data)
    test_data.append(data.values)
  return np.stack(test_data)

def compute_tp_fp(output, outputRef):
  '''
  computes true positive and false positive
  '''
  tp, fp = [], []
  threshold = np.arange(0,1.0001,.0001)
  for thr in threshold:
    decision = np.zeros((len(outputRef),))
    decision[output>thr] = 1
    # count TP
    temp = sum(decision[outputRef==1]==1)/sum(outputRef==1)
    tp.append(temp)
    temp = sum(decision[outputRef==0]==1)/sum(outputRef==0)
    fp.append(temp)
  return tp, fp

# read train-val lists
dataset_types = ['cambridge','dicova','epfl']
# make train pos neg list for each type
path_feats = '/content/feats/'
file_list = {}
for key in dataset_types:
  file_list[key+'_pos'] = glob.glob(path_feats + key + '/' + '*_pos.p')
  file_list[key+'_neg'] = glob.glob(path_feats + key + '/' + '*_neg.p')

# read and make cambridge test
path_feats = '/content/cambridge_test/'
test_list = glob.glob(path_feats + '*.p')
test_X = make_test(test_list)

test_X.shape

loop_val_auc = []
for iter in range(1):
  loop_val_auc.append([])
  train_X = {}
  train_Y = {}

  val_X = {}
  val_Y = {}

  # load train, val split data for each dataset
  for dataset_type in dataset_types:
    train_X[dataset_type], val_X[dataset_type], train_Y[dataset_type], val_Y[dataset_type] = \
            make_train_val_split(dataset_type, file_list)

  val_auc = {}
  # cross-evaluation
  i = 0
  for dataset_type in dataset_types:
    loop_val_auc[iter].append([])
    scale = StandardScaler().fit(train_X[dataset_type])
    train_X_stand = scale.transform(train_X[dataset_type])
    # clf = RandomForestClassifier(max_depth=6, random_state=0, criterion='gini').fit(train_X_stand, train_Y[dataset_type])
    clf = RandomForestClassifier(max_depth=24, n_estimators=257, criterion='entropy', random_state=0, n_jobs=-1).fit(train_X_stand, train_Y[dataset_type])
    # clf = RandomForestClassifier(max_depth=24, min_samples_leaf=3, n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True, random_state=0).fit(train_X[dataset_type], train_Y[dataset_type])
    for key in dataset_types:
      val_X_stand = scale.transform(val_X[key])
      output_scores = clf.predict_proba(val_X_stand)[:,1]
      tp, fp = compute_tp_fp(output_scores, val_Y[key])
      val_auc[dataset_type+'_'+key] = auc(fp, tp)
      loop_val_auc[iter][i].append(val_auc[dataset_type+'_'+key])
    i = i + 1

  # pooled evaluation
  train_X_all = np.vstack((train_X['cambridge'], train_X['dicova'], train_X['epfl']))
  train_Y_all = np.hstack((train_Y['cambridge'], train_Y['dicova'], train_Y['epfl']))

  loop_val_auc[iter].append([])
  scale = StandardScaler().fit(train_X_all)
  train_X_stand = scale.transform(train_X_all)
  clf = RandomForestClassifier(max_depth=24, n_estimators=257, criterion='entropy', random_state=0, n_jobs=-1).fit(train_X_stand, train_Y_all)
  for key in dataset_types:
    val_X_stand = scale.transform(val_X[key])
    output_scores = clf.predict_proba(val_X_stand)[:,1]
    tp, fp = compute_tp_fp(output_scores, val_Y[key])
    val_auc['pooled'+'_'+key] = auc(fp, tp)
    loop_val_auc[iter][i].append(val_auc['pooled'+'_'+key])

# to train a system on cambridge and test on the cambridge only
dataset_type = 'cambridge'
train_X = {}
train_Y = {}

val_X = {}
val_Y = {}

train_X[dataset_type], val_X[dataset_type], train_Y[dataset_type], val_Y[dataset_type] = \
            make_train_val_split(dataset_type, file_list)
scale = StandardScaler().fit(train_X[dataset_type])
train_X_stand = scale.transform(train_X[dataset_type])
clf = RandomForestClassifier(max_depth=24, n_estimators=257, criterion='entropy', random_state=0, n_jobs=-1).fit(train_X_stand, train_Y[dataset_type])

test_X_stand = scale.transform(test_X)
output_scores = clf.predict_proba(test_X_stand)[:,1]

plt.plot(output_scores)

plt.plot(output_scores)

plt.plot(output_scores)

df = {}
df['filename'] = []
df['prediction'] = []
thres = 0.3
for i in range(len(output_scores)):
  df['filename'].append(test_list[i].split('/')[-1].split('.p')[0])
  if output_scores[i] >thres:
    df['prediction'].append('positive')
  else:
    df['prediction'].append('negative')

new = pd.DataFrame.from_dict(df)
new_1 = new.sort_values(by = 'filename')
new_1.to_csv("cambridge_test_scores.csv", index=False)

new_1

# plot decisions at 80% sensitivity
temp = np.array(loop_val_auc)
fig = plt.subplots(figsize=(7,6.5))
ax = plt.subplot(1,1,1)
sns.set(font_scale=1.0)#for label size
FS = 12
x_label = ['D-1', 'D-2','D-3']
y_label = ['D-1', 'D-2', 'D-3', 'D-All']

sns.heatmap(np.mean(temp,axis=0)*100, annot=True, fmt='.3g', cmap='Blues', annot_kws={"size": 16},\
            cbar_kws={'label': 'AVG. AUC'})# font size
ax.set_xticks(np.arange(len(x_label))+.5)
ax.set_yticks(np.arange(len(y_label))+.5)
ax.set_xticklabels(x_label,rotation=0,fontsize=FS)
ax.set_yticklabels(y_label,rotation=90,fontsize=FS)
ax.figure.savefig("avg_auc.pdf", bbox_inches='tight')
plt.show()
sns.set() # Use seaborn's default style to make attractive graphs
sns.set_style("white")
sns.set_style("ticks")

# plot decisions at 80% sensitivity
temp = np.array(loop_val_auc)
fig = plt.subplots(figsize=(7,6.5))
ax = plt.subplot(1,1,1)
sns.set(font_scale=1.0)#for label size
FS = 12
x_label = ['D-1', 'D-2','D-3']
y_label = ['D-1', 'D-2', 'D-3', 'D-All']

sns.heatmap(np.std(temp,axis=0)*100, annot=True, fmt='.3g', cmap='Blues', annot_kws={"size": 16},\
            cbar_kws={'label': 'AVG. AUC'})# font size
ax.set_xticks(np.arange(len(x_label))+.5)
ax.set_yticks(np.arange(len(y_label))+.5)
ax.set_xticklabels(x_label,rotation=0,fontsize=FS)
ax.set_yticklabels(y_label,rotation=90,fontsize=FS)
ax.figure.savefig("avg_auc.pdf", bbox_inches='tight')
plt.show()
sns.set() # Use seaborn's default style to make attractive graphs
sns.set_style("white")
sns.set_style("ticks")

# train and cross-evaluate across datasets
val_auc = {}
for dataset_type in dataset_types:
  scale = StandardScaler().fit(train_X[dataset_type])
  train_X_stand = scale.transform(train_X[dataset_type])
  # clf = RandomForestClassifier(max_depth=6, random_state=0, criterion='gini').fit(train_X_stand, train_Y[dataset_type])
  clf = RandomForestClassifier(max_depth=24, n_estimators=257, criterion='entropy', random_state=0).fit(train_X_stand, train_Y[dataset_type])
  # clf = RandomForestClassifier(max_depth=24, min_samples_leaf=3, n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True, random_state=0).fit(train_X[dataset_type], train_Y[dataset_type])
  for key in dataset_types:
    val_X_stand = scale.transform(val_X[key])
    output_scores = clf.predict_proba(val_X_stand)[:,1]
    tp, fp = compute_tp_fp(output_scores, val_Y[key])
    val_auc[dataset_type+'_'+key] = auc(fp, tp)

# pooled evaluation
train_X_all = np.vstack((train_X['cambridge'], train_X['dicova'], train_X['epfl']))
train_Y_all = np.hstack((train_Y['cambridge'], train_Y['dicova'], train_Y['epfl']))

train_X_stand = scale.transform(train_X_all)
clf = RandomForestClassifier(max_depth=24, n_estimators=257, criterion='entropy', random_state=0).fit(train_X_stand, train_Y_all)
for key in dataset_types:
  val_X_stand = scale.transform(val_X[key])
  output_scores = clf.predict_proba(val_X_stand)[:,1]
  tp, fp = compute_tp_fp(output_scores, val_Y[key])
  val_auc['pooled'+'_'+key] = auc(fp, tp)

for i in range(4):
  for j in range(3):

val_auc

val_auc

plt.plot(clf.feature_importances_)

plt.plot(clf.feature_importances_)



https://osf.io/k8t23/download